ðŸ“‹ Checklist de MigraciÃ³n
Pre-Refactoring

 Backup completo del cÃ³digo actual
 Test suite completa para comportamiento actual
 Benchmarks de performance baseline
 DocumentaciÃ³n de casos de uso crÃ­ticos

Durante Refactoring

 Implementar por fases con backwards compatibility
 Feature flags para rollback rÃ¡pido
 Monitoring adicional durante transiciÃ³n
 Tests de regresiÃ³n continuos

Post-Refactoring

 Performance comparison vs baseline
 Memory usage analysis
 Load testing con arquitectura nueva
 DocumentaciÃ³n actualizada
 Training del equipo en nueva arquitectura

---

## ðŸŽ¯ Model Selection Strategy - Architecture Design

### Problem Identified (2025-09-01)
- Task analyzer overrides user's model selection
- VRAM constraints on RTX 4060 (8GB) cause model loading failures
- Need scalable solution for local â†’ API migration

### Solution Architecture

#### Hierarchy of Decision (Priority Order):
1. **USER REQUEST** (highest priority)
   - If `request.model` specified â†’ Use exactly that model
   - Respect user choice always

2. **RESOURCE MONITOR** (physical constraints)
   - VRAM < 3GB â†’ Force lightweight model or API fallback
   - VRAM > 6GB â†’ Allow large models
   - Current GPU: RTX 4060 = 8GB total

3. **TASK ANALYZER** (intelligent recommendation)
   - Only if NO model specified by user
   - task=code â†’ deepseek7b (specialized for coding)
   - task=chat â†’ mistral7b (balanced performance)
   - task=analysis â†’ llama3 (reasoning capabilities)

4. **FALLBACK CHAIN**
   - Local GPU â†’ Local CPU â†’ Remote API
   - Graceful degradation with performance tracking

### Implementation Phases

#### Phase 1: Local Testing (Current - RTX 4060)
```python
# Conservative strategy for limited VRAM
def smart_model_selection(request_model, available_vram, task_type):
    if request_model:
        return request_model if can_fit_in_vram() else fallback_to_api()
    return "mistral7b"  # Safe default for all tasks
```

#### Phase 2: API Scaling (Future)
```python
provider_priority = [
    "local_gpu",     # RTX 4060 for lightweight models
    "openai_api",    # Complex reasoning tasks
    "anthropic_api", # Deep analysis
    "local_cpu"      # Last resort
]
```

### Quick Fix Implementation
- Modify task_analyzer to respect `state.get("requested_model")`
- Use mistral7b as universal default (VRAM conservative)
- Maintain architecture for future API integration

### Files to Modify:
- [ ] `api/endpoints/orchestrator.py` - Pass model parameter
- [ ] `langchain_integration/langgraph/routing_agent.py` - Accept model param
- [ ] `langchain_integration/langgraph/agent_state.py` - Add requested_model field
- [ ] `langchain_integration/langgraph/nodes/task_analyzer_node.py` - Respect user model

### Benefits:
âœ… Immediate VRAM issue resolution
âœ… User control maintained
âœ… Scalable architecture for API migration
âœ… Backward compatible 