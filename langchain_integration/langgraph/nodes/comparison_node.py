# langchain_integration/langgraph/nodes/comparison_node.py
from langchain_integration.langgraph.agent_state import AgentState
from langchain_integration.langgraph.local_llm_node import build_local_llm_tool_node

def comparison_node(state: AgentState) -> AgentState:
    print("âš–ï¸ Comparando respuestas...")
    messages = state.get("messages", [])

    llm = build_local_llm_tool_node(
        model_key=state["selected_model"],
        strategy=state["strategy"],
        max_tokens=512
    )

    prompt = f"""Dada la siguiente pregunta y dos respuestas de distintos modelos, decide cuÃ¡l es mejor y por quÃ©. Usa criterios de claridad, correcciÃ³n tÃ©cnica y utilidad.

â“ Pregunta:
{state["input"]}

ğŸ…°ï¸ Respuesta A:
{state.get("output_a", "(sin definir)")}

ğŸ…±ï¸ Respuesta B:
{state.get("output_b", "(sin definir)")}

Indica cuÃ¡l preferÃ­s y justifica la elecciÃ³n brevemente.
"""

    try:
        result = llm.invoke(prompt)
        messages.append("ComparaciÃ³n completada.")
        return {
            **state,
            "output": result,
            "messages": messages
        }
    except Exception as e:
        messages.append(f"Error en comparaciÃ³n: {str(e)}")
        return {
            **state,
            "output": "",
            "messages": messages
        }