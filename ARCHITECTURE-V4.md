# AI Agent Lab - Architecture V4

> **Estado:** DocumentaciÃ³n actualizada y verificada basada en cÃ³digo real  
> **Fecha:** Septiembre 2024  
> **VersiÃ³n anterior:** [ARCHITECTURE-V3.md](docs/archive/ARCHITECTURE-V3.md) (histÃ³rico)

---

## ğŸ¯ Executive Summary

**AI Agent Lab V4** es un sistema de orquestaciÃ³n de LLMs locales con flujos configurables de deliberaciÃ³n entre modelos.

### **Â¿QuÃ© hace hoy?**
- âœ… **Inferencia local** con modelos HuggingFace (Mistral, Llama3, DeepSeek)
- âœ… **Challenge Flow** - Debate automÃ¡tico entre 3 LLMs (Creatorâ†’Challengerâ†’Refiner)
- âœ… **Linear Flow** - Pipeline tradicional con anÃ¡lisis, validaciÃ³n y contexto histÃ³rico
- âœ… **API REST** para integraciÃ³n frontend/backend
- âœ… **Provider Gateway** para abstracciÃ³n de modelos
- âœ… **Cache inteligente** y gestiÃ³n de recursos GPU

### **Â¿QuÃ© NO hace?**
- âŒ **Providers remotos** (OpenAI, Anthropic) - solo referencias de cÃ³digo
- âŒ **Multi-Perspective Flow** - configurado pero no probado
- âŒ **Frontend integrado** - desconectado tras refactor V3
- âŒ **Persistencia de conversaciones** - solo outputs individuales

### **Â¿Por quÃ© V4?**
V3 tenÃ­a aspiraciones no implementadas mezcladas con funcionalidad real. V4 documenta **exactamente lo que existe y funciona** hoy.

---

## ğŸš§ Scope & Non-Goals

### **âœ… In Scope (V4)**
- OrquestaciÃ³n de modelos locales HuggingFace
- Challenge Flow para deliberaciÃ³n LLM
- API REST estable para inferencia
- GestiÃ³n de recursos GPU y VRAM
- Sistema de nodos modulares LangGraph

### **âŒ Non-Goals / Deprecated**
- **Providers remotos** â†’ Futuro (V5+)
- **Multi-Perspective Flow** â†’ Experimental, no production-ready  
- **AutoGen integration** â†’ Archivado en `/archive/`
- **Direct inference scripts** â†’ Referencias inexistentes removidas
- **Strategy registry** â†’ Usando `constants/strategies.py` (legacy pero funcional)

---

## ğŸ—ï¸ Modules & Ownership

### **ğŸ§ª Lab Core (Local Execution)**
- **Purpose:** GestiÃ³n de modelos locales y ejecuciÃ³n
- **Entrypoints:** `local_models/model_manager.py`, `local_models/model_executor.py`
- **Owner:** Sistema de cache y lifecycle de modelos HF
- **Status:** âœ… Estable, production-ready

### **ğŸ”„ Orchestration Engine (LangGraph)**
- **Purpose:** Flujos configurables entre mÃºltiples LLMs
- **Entrypoints:** `langchain_integration/langgraph/routing_agent.py`
- **Owner:** Graph building, flow execution, metrics
- **Status:** âœ… Challenge Flow probado, Linear Flow estable

### **ğŸŒ‰ Provider Gateway**
- **Purpose:** AbstracciÃ³n unificada para diferentes providers
- **Entrypoints:** `providers/provider_gateway.py`
- **Owner:** Request routing, model resolution
- **Status:** âœ… Local provider operativo, remote providers pendientes

### **ğŸ”Œ REST API (FastAPI)**
- **Purpose:** Endpoints para frontend y integraciÃ³n externa
- **Entrypoints:** `api/server.py`
- **Owner:** HTTP interface, request/response handling
- **Status:** âœ… Funcional, endpoints `/inference` y `/orchestrate`

### **ğŸ–¥ï¸ Frontend (Desconectado)**
- **Purpose:** UI para interacciÃ³n con flujos
- **Status:** âš ï¸ Existe pero desconectado tras refactor V3
- **Next:** Requiere reconexiÃ³n con nuevos endpoints

---

## ğŸ“‹ Contratos

### **Unified Inference Request**
```typescript
interface InferenceRequest {
  prompt: string
  model?: "mistral7b" | "llama3" | "deepseek7b" | "deepseek-coder"  // default: "mistral7b"
  strategy?: "standard" | "optimized" | "streaming"                 // default: "optimized"
  flow_type?: "linear" | "challenge"                                // default: "linear"
  max_tokens?: number                                               // default: 1024
  temperature?: number                                              // default: 0.7
}
```

### **Unified Response**
```typescript
interface InferenceResponse {
  success: boolean
  output: string
  metrics: {
    total_time: number           // milliseconds
    cache_hit: boolean
    tokens_generated: number
    model_used: string
    source: "local"              // future: "openai" | "anthropic"
  }
  flow?: {                       // only for orchestrated flows
    type: string
    nodes: FlowNode[]
    execution_id: string
  }
  error?: ErrorInfo
}
```

### **Error Handling**
```typescript
interface ErrorInfo {
  code: "MODEL_NOT_FOUND" | "VRAM_INSUFFICIENT" | "GENERATION_FAILED" | "VALIDATION_FAILED"
  message: string              // human-readable Spanish/English
  details?: object            // technical details for debugging
}
```

---

## ğŸ”„ CatÃ¡logo de Flujos Vigentes

### **1. Linear Flow** âœ… **Production Ready**
```
TaskAnalyzer â†’ ResourceMonitor â†’ ExecutionNode â†’ OutputValidator â†’ HistoryReader â†’ SummaryNode
```
- **Uso:** Inferencia tradicional con contexto y validaciÃ³n
- **Tiempo tÃ­pico:** 30-60 segundos
- **Casos probados:** Chat, anÃ¡lisis tÃ©cnico, generaciÃ³n de cÃ³digo

### **2. Challenge Flow** âœ… **Production Ready**
```
Creator â†’ Challenger â†’ Refiner
```
- **Uso:** DeliberaciÃ³n automÃ¡tica entre modelos para mejorar calidad
- **Tiempo tÃ­pico:** 90 segundos (3 modelos secuenciales)
- **Casos probados:** ValidaciÃ³n de contraseÃ±as, funciones Python, anÃ¡lisis de seguridad
- **Persistencia:** JSON en `outputs/challenge_flows/{execution_id}.json`

### **3. Multi-Perspective Flow** ğŸš« **No Production Ready**
```
Input â†’ [Security Expert, Performance Expert, UX Expert] â†’ Synthesizer
```
- **Status:** Configurado pero no completamente probado
- **Blocker:** Necesita testing exhaustivo y debugging
- **Roadmap:** Candidato para V4.1

---

## ğŸš€ Run/Operate

### **Variables de Entorno**
```bash
# Core
APP_ENV=dev                    # dev | prod
LOG_LEVEL=INFO                 # DEBUG | INFO | WARN | ERROR
PORT=8001

# Local Models  
MAX_VRAM_USAGE_GB=6.0         # GPU memory limit
MODEL_CACHE_DIR=.cache/models # HF models cache

# API Keys (future)
OPENAI_API_KEY=               # not implemented yet
ANTHROPIC_API_KEY=            # not implemented yet
```

### **Startup**
```bash
# 1. Environment
python -m venv venv
venv\Scripts\activate          # Windows
# source venv/bin/activate     # Linux/Mac

# 2. Dependencies
pip install -r requirements.txt

# 3. Start API server
python api/server.py

# 4. Health check
curl http://localhost:8001/health
```

### **Verificar Salud del Sistema**
```bash
# API status
GET http://localhost:8001/health

# GPU status  
GET http://localhost:8001/models

# Test linear flow
POST http://localhost:8001/api/inference
{
  "prompt": "Hello world",
  "model": "mistral7b"
}

# Test challenge flow
POST http://localhost:8001/api/orchestrate  
{
  "prompt": "Create a password validator",
  "flow_type": "challenge"
}
```

---

## ğŸ—‘ï¸ Deprecations

### **Removido en V4:**
- `scripts/direct_inference.py` â†’ **Nunca existiÃ³**, referencias eliminadas
- `providers/remote/` â†’ **No implementado**, movido a roadmap futuro
- `examples/` directory â†’ **Archivado** en `/archive/examples/`
- `constants/models.py` hardcoded â†’ **Migrado** a `providers/registries/model_registry.py`

### **Mantenido (Legacy pero funcional):**
- `constants/strategies.py` â†’ DeberÃ­a ser `providers/registries/strategy_registry.py` pero funciona
- `local_models/` directory â†’ DeberÃ­a estar en `providers/local/manager/` pero funciona

### **Frontend Disconnected:**
- Interfaz web existe pero no conectada a nuevos endpoints V3
- Requiere trabajo de reconexiÃ³n (no roto, solo desconectado)

---

## ğŸ—“ï¸ Roadmap Corto (4-6 semanas)

### **Sprint 1 (Semana 1-2): StabilizaciÃ³n**
1. **Reconectar Frontend** - Actualizar UI para usar endpoints V3 âœ…
2. **Multi-Perspective Flow Testing** - Debugging y casos de prueba ğŸ”§
3. **Error Handling Unificado** - Implementar cÃ³digos de error estÃ¡ndar ğŸ“‹

### **Sprint 2 (Semana 3-4): Extensibilidad**  
4. **Strategy Registry Migration** - `constants/strategies.py` â†’ `providers/registries/` ğŸ”„
5. **JSON Flow Configs** - Migrar de Python configs a JSON declarativo ğŸ“„
6. **Flow Validation Framework** - Testing automatizado de nuevos flujos âœ…

### **Sprint 3 (Semana 5-6): Remote Providers**
7. **OpenAI Adapter** - Implementar `providers/remote/openai_provider.py` ğŸŒ
8. **Fallback Logic** - Local â†’ Remote fallback automÃ¡tico ğŸ”„

### **Criterio de Ã‰xito V4:**
- âœ… Frontend reconectado y funcional
- âœ… Multi-Perspective Flow production-ready  
- âœ… Al menos 1 provider remoto implementado
- âœ… DocumentaciÃ³n V4 publicada y linkeada en README

---

## ğŸ“š Documentation Structure

```
ai-agent-lab/
â”œâ”€â”€ ARCHITECTURE-V4.md           # Este documento
â”œâ”€â”€ README.md                    # Actualizado con link V4
â””â”€â”€ docs/
    â”œâ”€â”€ NODES-SUMMARY.md         # CatÃ¡logo de nodos
    â”œâ”€â”€ FLOWS-CATALOG.md         # GuÃ­a visual de flujos  
    â”œâ”€â”€ ARCHITECTURE-CORRECTIONS.md # Discrepancias V3
    â””â”€â”€ archive/
        â””â”€â”€ ARCHITECTURE-V3.md   # HistÃ³rico
```

---

**Status:** V4 Draft Ready  
**Next Action:** Update README.md to reference V4  
**Validation:** Code-verified, production-tested features only